import re
from datetime import datetime
from pathlib import Path

import polars as pl
from bs4 import BeautifulSoup
from dateutil.parser import parse
from selenium import webdriver

from eed_webscrapping_scripts.modules import (
    ask_user_for_local_production_run,
    decrypt_direct,
    decrypt_file,
    save_webpage,
)
from eed_webscrapping_scripts.pollenvorhersage import (
    download_wepages,
    get_config,
    open_webpage_and_select_plz,
    prepare_db,
    upload_webpage_to_db,
)


class PollenvorhersageHandler:
    def __init__(self):
        self.cfg = get_config()
        self.con = prepare_db(self.cfg)

    def fetch_and_store_html(self):
        print("START")

        # set parameters
        cfg = self.cfg
        ask_user_for_local_production_run(cfg)
        url = decrypt_direct(cfg["pollenvorhersage"]["url"])
        plzs = [decrypt_direct(plz) for plz in cfg["pollenvorhersage"]["plz"]]
        print(len(plzs))

        con = self.con

        for _i_, plz in enumerate(plzs):
            print(f"{_i_=}")
            if cfg["env"]["_ENVIRONMENT_"] == "PROD":
                driver = webdriver.Chrome()
                driver = open_webpage_and_select_plz(url, plz, driver)
                file_rel = Path("pollenvorhersage", "websites", f"{plz}.html")
                file = Path(cfg["git_root"], file_rel)
                save_webpage(driver.page_source, str(file))
            else:
                # do not access website, use encrypted webpage instead
                file_rel_encrypted = Path("pollenvorhersage", "websites", "encrypted_website.html")
                file_encrypted = Path(cfg["git_root"], file_rel_encrypted)
                file_rel_decrypted = Path("pollenvorhersage", "websites", "decrypted_website.html")
                file = Path(cfg["git_root"], file_rel_decrypted)
                decrypt_file(file_encrypted, file)

            upload_webpage_to_db(con, file, plz, cfg)
            print("upladed")

        # clean up
        match cfg["env"]["_ENVIRONMENT_"]:
            case "PROD":
                driver.quit()
                con.close()
            case "DEV":
                pass

        print("END")
        return con

    def extract_pollenvorhersage(self):
        cfg = self.cfg
        con = self.con
        webpages = download_wepages(cfg=cfg, con=con)
        print(webpages)

        current_date = datetime.now().date()
        for i in range(len(webpages)):
            content = webpages["content"][i]
            soup = BeautifulSoup(content, "html.parser")
            dates_to_extract = soup.find_all(class_="datum")
            dates = list(range(len(dates_to_extract)))
            for j in range(len(dates_to_extract)):
                date_to_extract = dates_to_extract[j].get_text(strip=True)
                dd_mm = re.search(r"\d{2}.\d{2}", date_to_extract)[0]
                parsed_date = parse(f"""{dd_mm}.{current_date.year}""", dayfirst=True).date()
                dates[j] = parsed_date
            dates_str = [date.strftime("%Y_%m_%d") for date in dates]
            soup_pollenarten = soup.find_all(class_="tooltip")
            pollenarten = list(range(len(soup_pollenarten)))
            for k in range(len(soup_pollenarten)):
                pollenart = soup_pollenarten[k].find(class_="tooltiptext").find("img")["alt"]
                pollenarten[k] = pollenart

            soup_belastungen = soup.find_all("img", {"title": True})
            belastungen = [
                belastung["title"]
                for belastung in soup_belastungen[: (len(dates) * len(soup_pollenarten))]
            ]
            reshaped_values = [
                belastungen[i : i + len(dates)] for i in range(0, len(belastungen), len(dates))
            ]

            df = (
                pl.DataFrame(reshaped_values, schema=dates_str)
                .with_columns(pl.Series("pollenart", pollenarten))
                .unpivot(index="pollenart")
            )
            print(len(df))
            mapping = {
                "keine Belastung": 0,
                "schwache Belastung": 1,
                "mittlere Belastung": 2,
                "hohe Belastung": 3,
            }
            con.sql(f"""
                -- CREATE OR REPLACE TEMP TABLE pollenflug AS
                SELECT
                    * EXCLUDE(variable)
                    , strptime(variable, '%Y_%m_%d')::DATE AS date
                    , MAP {str(mapping)}[value] AS belastung
                FROM df
                ORDER BY pollenart, date
            """)
        # Wait for the data to load and scrape the data
        # Add your scraping logic here


if __name__ == "__main__":
    pollenvorhersage_handler = PollenvorhersageHandler()
    con = pollenvorhersage_handler.fetch_and_store_html()
